{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pickle\n",
    "\n",
    "markers = ['o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X']\n",
    "colors = [\n",
    "    '#377eb8',\n",
    "    '#e41a1c',\n",
    "    '#4daf4a',\n",
    "    '#984ea3',\n",
    "    '#ff7f00',\n",
    "    '#ffff33',\n",
    "    '#a65628',\n",
    "    '#f781bf',\n",
    "    '#999999',\n",
    "]\n",
    "\n",
    "graph_names = [\n",
    "    # social\n",
    "    'OF',\n",
    "    'openflights',\n",
    "    # hypergraphs\n",
    "    'coauth-DBLP-proj-graph',\n",
    "    'coauth-MAG-Geology-proj-graph',\n",
    "    'threads-ask-ubuntu-proj-graph',\n",
    "    'threads-math-sx-proj-graph',\n",
    "    'threads-stack-overflow-proj-graph',\n",
    "    # temporal\n",
    "    'sx-askubuntu',\n",
    "    'sx-mathoverflow',\n",
    "    'sx-stackoverflow',\n",
    "    'sx-superuser',\n",
    "]\n",
    "\n",
    "graph_names_short = [\n",
    "    # social\n",
    "    'OF',\n",
    "    'FL',\n",
    "    # hypergraphs\n",
    "    'co-DB',\n",
    "    'co-GE',\n",
    "    'th-UB',\n",
    "    'th-MA',\n",
    "    'th-SO',\n",
    "    # temporal\n",
    "    'sx-UB',\n",
    "    'sx-MA',\n",
    "    'sx-SO',\n",
    "    'sx-SU',\n",
    "]\n",
    "\n",
    "name2nameShort = dict(zip(graph_names, graph_names_short))\n",
    "\n",
    "g2fitting = {\n",
    "    # social\n",
    "    'OF': 1,\n",
    "    'openflights': 2,\n",
    "    # hypergraphs\n",
    "    'coauth-DBLP-proj-graph': 3,\n",
    "    'coauth-MAG-Geology-proj-graph': 3,\n",
    "    'threads-ask-ubuntu-proj-graph': 1,\n",
    "    'threads-math-sx-proj-graph': 1,\n",
    "    'threads-stack-overflow-proj-graph': 1,\n",
    "    # temporal\n",
    "    'sx-askubuntu': 2,\n",
    "    'sx-mathoverflow': 2,\n",
    "    'sx-stackoverflow': 2,\n",
    "    'sx-superuser': 2,\n",
    "}\n",
    "\n",
    "i2ak = {\n",
    "    1: (0.7, 1.3),\n",
    "    2: (0.9, 1.1),\n",
    "    3: (0.99, 1.05)\n",
    "}\n",
    "\n",
    "g2nm = {\n",
    "    'OF': (897, 71380),\n",
    "    'openflights': (2905, 15645),\n",
    "\n",
    "    'coauth-DBLP-proj-graph': (1654109, 7713116),\n",
    "    'coauth-MAG-Geology-proj-graph': (898648, 4891112),\n",
    "    'threads-ask-ubuntu-proj-graph': (82075, 182648),\n",
    "    'threads-math-sx-proj-graph': (152702, 1088735),\n",
    "    'threads-stack-overflow-proj-graph': (2301070, 20989078),\n",
    "\n",
    "    'sx-askubuntu': (152599, 453221),\n",
    "    'sx-mathoverflow': (24668, 187939),\n",
    "    'sx-stackoverflow': (2572345, 28177464),\n",
    "    'sx-superuser': (189191, 712870),\n",
    "}\n",
    "\n",
    "gt_c_star_wt1 = {\n",
    "    'OF': 241,\n",
    "    'openflights': 64,\n",
    "\n",
    "    'coauth-DBLP-proj-graph': 83,\n",
    "    'coauth-MAG-Geology-proj-graph': 74,\n",
    "    'threads-ask-ubuntu-proj-graph': 73,\n",
    "    'threads-math-sx-proj-graph': 372,\n",
    "    'threads-stack-overflow-proj-graph': 685,\n",
    "\n",
    "    'sx-askubuntu': 152,\n",
    "    'sx-mathoverflow': 185,\n",
    "    'sx-stackoverflow': 886,\n",
    "    'sx-superuser': 202,\n",
    "}\n",
    "\n",
    "gt_c_star_more = {\n",
    "    'OF': [(241, 271), (190, 192), (157, 156), (134, 137), (119, 101)],\n",
    "    'openflights': [(64, 66), (31, 31), (17, 17), (None, None), (None, None)],\n",
    "    'coauth-DBLP-proj-graph': [(83, 88), (36, 29), (22, 24), (20, 21), (16, 16)],\n",
    "    'coauth-MAG-Geology-proj-graph': [(74, 92), (52, 49), (34, 40), (28, 30), (24, 21)],\n",
    "    'threads-ask-ubuntu-proj-graph': [(73, 87), (30, 31), (19, 20), (18, 11), (15, 11)],\n",
    "    'threads-math-sx-proj-graph': [(372, 401), (145, 153), (114, 114), (84, 67), (63, 59)],\n",
    "    'threads-stack-overflow-proj-graph': [(685, 750), (208, 205), (134, 129), (97, 82), (74, 72)],\n",
    "    'sx-askubuntu': [(152, 149), (63, 69), (48, 42), (36, 27), (31, 22)],\n",
    "    'sx-mathoverflow': [(185, 181), (113, 102), (75, 63), (60, 49), (51, 41)],\n",
    "    'sx-stackoverflow': [(886, 749), (407, 324), (221, 203), (169, 130), (120, 103)],\n",
    "    'sx-superuser': [(202, 206), (96, 93), (63, 54), (48, 37), (36, 27)],\n",
    "}\n",
    "\n",
    "p_data = Path(f'data')\n",
    "p_data.mkdir(exist_ok=True)\n",
    "\n",
    "p_results = Path('results')\n",
    "p_results.mkdir(exist_ok=True)\n",
    "\n",
    "Edge = tuple[int, int]\n",
    "EdgeAndWeight = tuple[int, int, int]\n",
    "graphs_sorted_m = sorted(graph_names, key=lambda xx: g2nm[xx][1])\n",
    "\n",
    "\n",
    "def iter_edges(input_graph, with_weight=False, desc='edges'):\n",
    "    return tqdm(input_graph.edges.data('weight', default=1) if with_weight else input_graph.edges,\n",
    "                total=input_graph.number_of_edges(), leave=False, desc=desc)\n",
    "\n",
    "\n",
    "def iter_nodes(input_graph, desc='nodes'):\n",
    "    return tqdm(input_graph.nodes, total=input_graph.number_of_nodes(), leave=False, desc=desc)\n",
    "\n",
    "\n",
    "def data_exist(ds, data_name_, layer_index=None):\n",
    "    if layer_index is not None:\n",
    "        return (p_data / data_name_ / f'{ds}.{data_name_}_layer{layer_index}').is_file()\n",
    "    return (p_data / data_name_ / f'{ds}.{data_name_}').is_file()\n",
    "\n",
    "\n",
    "def data_file_path(ds, data_name_, layer_index=None, write=False):\n",
    "    if write:\n",
    "        (p_data / data_name_).mkdir(exist_ok=True)\n",
    "        mode = 'wb'\n",
    "    else:\n",
    "        mode = 'rb'\n",
    "    if layer_index is not None:\n",
    "        return p_data / data_name_ / f'{ds}.{data_name_}_layer{layer_index}', mode\n",
    "    return p_data / data_name_ / f'{ds}.{data_name_}', mode\n",
    "\n",
    "\n",
    "def save_data(data, ds, data_name_, layer_index=None):\n",
    "    with open(*data_file_path(ds, data_name_, write=True, layer_index=layer_index)) as f_:\n",
    "        pickle.dump(data, f_)\n",
    "\n",
    "\n",
    "def load_data(ds, data_name_, layer_index=None):\n",
    "    with open(*data_file_path(ds, data_name_, write=False, layer_index=layer_index)) as f_:\n",
    "        return pickle.load(f_)\n",
    "\n",
    "\n",
    "def min_max_tuple(xx, yy):\n",
    "    return min(xx, yy), max(xx, yy)\n",
    "\n",
    "\n",
    "def reorder_nodes(input_graph):\n",
    "    return nx.convert_node_labels_to_integers(input_graph)\n",
    "\n",
    "\n",
    "def take_gcc(input_graph):\n",
    "    return input_graph.subgraph(max(nx.connected_components(input_graph), key=len))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fsolve\n",
    "import math\n",
    "from itertools import combinations, product\n",
    "\n",
    "# PEAR\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_PEAR = p_experiments / 'PEAR'\n",
    "p_experiments_PEAR.mkdir(exist_ok=True)\n",
    "\n",
    "random_seeds = [1, 2, 3]\n",
    "\n",
    "for graph_name, random_seed in product(graphs_sorted_m, random_seeds):\n",
    "    np.random.seed(random_seed)\n",
    "    print(graph_name)\n",
    "    a, k = i2ak[g2fitting[graph_name]]\n",
    "    graph_name_short = name2nameShort[graph_name]\n",
    "    G = load_data(graph_name, 'graph')\n",
    "    e2predWeight = dict()\n",
    "    for u, v in iter_edges(G):\n",
    "        e2predWeight[min_max_tuple(u, v)] = 1\n",
    "\n",
    "    for i_layer in range(2, 6):\n",
    "        v2Nv = {v: set(G[v]) for v in G}\n",
    "        cn2p = defaultdict(int)\n",
    "        cn2m = defaultdict(int)\n",
    "        e2cn = dict()\n",
    "        n, m = G.number_of_nodes(), G.number_of_edges()\n",
    "        print(graph_name_short, f'layer-{i_layer}', G)\n",
    "        if i_layer == 2:\n",
    "            tilde_c_star = gt_c_star_wt1[graph_name]\n",
    "            for u, v in iter_edges(G):\n",
    "                Nu, Nv = v2Nv[u], v2Nv[v]\n",
    "                cn_uv = len(Nu & Nv)\n",
    "                cn2m[cn_uv] += 1\n",
    "                e2cn[min_max_tuple(u, v)] = cn_uv\n",
    "        else:\n",
    "            for u, v in tqdm(combinations(v2Nv, 2), total=math.comb(n, 2)):\n",
    "                Nu, Nv = v2Nv[u], v2Nv[v]\n",
    "                cn_uv = len(Nu & Nv)\n",
    "                cn2p[cn_uv] += 1\n",
    "                if v in Nu:\n",
    "                    cn2m[cn_uv] += 1\n",
    "                    e2cn[min_max_tuple(u, v)] = cn_uv\n",
    "            try:\n",
    "                tilde_c_star = min(c for c in cn2p if 0 < cn2p[c] == cn2m[c])\n",
    "            except ValueError:\n",
    "                break\n",
    "        print(f'tilde_c_star = {tilde_c_star}')\n",
    "\n",
    "\n",
    "        # compute number of strong edges\n",
    "        def solve_se(se_input: float) -> float:\n",
    "            LHS = a * (se_input / m) ** k\n",
    "            RHS_num = se_input - sum(cn2m[c] * min(1., c / tilde_c_star) for c in cn2m)\n",
    "            RHS_den = sum(cn2m[c] * (tilde_c_star - c) / tilde_c_star for c in cn2m if c <= tilde_c_star)\n",
    "            RHS = RHS_num / RHS_den\n",
    "            return LHS - RHS\n",
    "\n",
    "\n",
    "        se_root = float(fsolve(solve_se, np.array(0.1 * m))[0])\n",
    "        print(f'number of SEs = {se_root}, numerical difference = {solve_se(se_root):.4f}')\n",
    "\n",
    "        # compute strong fractions\n",
    "        cn2sf = defaultdict(float)\n",
    "        p0 = cn2sf[0] = a * (se_root / m) ** k\n",
    "        for c in cn2m:\n",
    "            if c:\n",
    "                cn2sf[c] = min(1., p0 + (1 - p0) * c / tilde_c_star)\n",
    "\n",
    "        # sampling\n",
    "        strong_edges = []\n",
    "        for e, cn_e in e2cn.items():\n",
    "            p_e = cn2sf[cn_e]\n",
    "            if np.random.random() <= p_e:\n",
    "                strong_edges.append(e)\n",
    "                e2predWeight[e] = i_layer\n",
    "        G = nx.from_edgelist(strong_edges)\n",
    "        # save\n",
    "        with open(p_experiments_PEAR / f'{graph_name}-seed{random_seed}.G_{i_layer}', 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "    with open(p_experiments_PEAR / f'{graph_name}-seed{random_seed}.e2predWeight', 'wb') as f:\n",
    "        pickle.dump(e2predWeight, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# PRD (purely random)\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_PRD = p_experiments / 'PRD'\n",
    "p_experiments_PRD.mkdir(exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "for graph_name in graphs_sorted_m:\n",
    "    print(graph_name)\n",
    "    Ei_list = []\n",
    "    for i_layer in range(2, 6):\n",
    "        G = load_data(graph_name, 'layers', layer_index=i_layer)\n",
    "        Ei_list.append(G.number_of_edges())\n",
    "    i_list = list(range(2, 6))\n",
    "    X = np.array(i_list)\n",
    "    Y = np.array(Ei_list)\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(np.log(X).reshape(-1, 1), np.log(Y))\n",
    "    slope = reg.coef_[0]\n",
    "    intercept = reg.intercept_\n",
    "    # print(f'fitting slope = {slope:.3f}, intercept = {intercept:.3f}')\n",
    "    Y_fitted = np.exp(reg.predict(np.log(X).reshape(-1, 1)))\n",
    "    Y_fitted = Y_fitted.reshape(-1)\n",
    "    i2Ei_fitted = {i: Ei_fitted for i, Ei_fitted in zip(i_list, Y_fitted)}\n",
    "    i2Ei_fitted[1] = g2nm[graph_name][1]\n",
    "    with open(*data_file_path(graph_name, 'edges')) as f:\n",
    "        edges: list[Edge] = pickle.load(f)\n",
    "    e2predWeight = dict()\n",
    "    for u, v in tqdm(edges):\n",
    "        e2predWeight[min_max_tuple(u, v)] = 1\n",
    "\n",
    "    for i_layer in i_list:\n",
    "        # sampling\n",
    "        number_SE = round(i2Ei_fitted[i_layer])\n",
    "        strong_edges_indices = np.random.choice(len(edges), number_SE, replace=False)\n",
    "        strong_edges = []\n",
    "        for ie in strong_edges_indices:\n",
    "            e = edges[ie]\n",
    "            strong_edges.append(e)\n",
    "            e2predWeight[e] = i_layer\n",
    "        G = nx.from_edgelist(strong_edges)\n",
    "        edges = strong_edges[:]\n",
    "        # save\n",
    "        with open(p_experiments_PRD / f'{graph_name}.G_{i_layer}', 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "\n",
    "    with open(p_experiments_PRD / f'{graph_name}.e2predWeight', 'wb') as f:\n",
    "        pickle.dump(e2predWeight, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# SCN (sorting-CN)\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_SCN = p_experiments / 'SCN'\n",
    "p_experiments_SCN.mkdir(exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "for graph_name in graphs_sorted_m:\n",
    "    print(graph_name)\n",
    "    Ei_list = []\n",
    "    for i_layer in range(2, 6):\n",
    "        G = load_data(graph_name, 'layers', layer_index=i_layer)\n",
    "        Ei_list.append(G.number_of_edges())\n",
    "    i_list = list(range(2, 6))\n",
    "    X = np.array(i_list)\n",
    "    Y = np.array(Ei_list)\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(np.log(X).reshape(-1, 1), np.log(Y))\n",
    "    slope = reg.coef_[0]\n",
    "    intercept = reg.intercept_\n",
    "    # print(f'fitting slope = {slope:.3f}, intercept = {intercept:.3f}')\n",
    "    Y_fitted = np.exp(reg.predict(np.log(X).reshape(-1, 1)))\n",
    "    Y_fitted = Y_fitted.reshape(-1)\n",
    "    i2Ei_fitted = {i: round(Ei_fitted) for i, Ei_fitted in zip(i_list, Y_fitted)}\n",
    "    i2Ei_fitted[1] = g2nm[graph_name][1]\n",
    "    edges = load_data(graph_name, 'edges')\n",
    "    number_of_CNs_list = load_data(graph_name, 'number_of_CNs_list')\n",
    "    edges_sorted_CN = [x for _, x in sorted(zip(number_of_CNs_list, edges), reverse=True)]\n",
    "    weights_sorted = list(chain.from_iterable([[i] * (i2Ei_fitted[i] - i2Ei_fitted.get(i + 1, 0))\n",
    "                                               for i in reversed(i_list)]))\n",
    "    e2predWeight = dict(zip(edges_sorted_CN, weights_sorted))\n",
    "    for i_layer in i_list:\n",
    "        # sampling\n",
    "        strong_edges = [e for e in edges if e2predWeight[e] >= i_layer]\n",
    "        G = nx.from_edgelist(strong_edges)\n",
    "        edges = strong_edges[:]\n",
    "        # save\n",
    "        with open(p_experiments_SCN / f'{graph_name}.G_{i_layer}', 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "\n",
    "    with open(p_experiments_SCN / f'{graph_name}.e2predWeight', 'wb') as f:\n",
    "        pickle.dump(e2predWeight, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from karateclub import RandNE\n",
    "\n",
    "# SEB (sort-embedding)\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_SEB = p_experiments / 'SEB'\n",
    "p_experiments_SEB.mkdir(exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "for graph_name in graphs_sorted_m:\n",
    "    print(graph_name)\n",
    "    Ei_list = []\n",
    "    for i_layer in range(2, 6):\n",
    "        G = load_data(graph_name, 'layers', layer_index=i_layer)\n",
    "        Ei_list.append(G.number_of_edges())\n",
    "    i_list = list(range(2, 6))\n",
    "    X = np.array(i_list)\n",
    "    Y = np.array(Ei_list)\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(np.log(X).reshape(-1, 1), np.log(Y))\n",
    "    slope = reg.coef_[0]\n",
    "    intercept = reg.intercept_\n",
    "    # print(f'fitting slope = {slope:.3f}, intercept = {intercept:.3f}')\n",
    "    Y_fitted = np.exp(reg.predict(np.log(X).reshape(-1, 1)))\n",
    "    Y_fitted = Y_fitted.reshape(-1)\n",
    "    i2Ei_fitted = {i: round(Ei_fitted) for i, Ei_fitted in zip(i_list, Y_fitted)}\n",
    "    i2Ei_fitted[1] = g2nm[graph_name][1]\n",
    "    edges = load_data(graph_name, 'edges')\n",
    "    G = load_data(graph_name, 'graph')\n",
    "    try:\n",
    "        embed_RandNE = load_data(graph_name, 'embed_RandNE')\n",
    "    except:\n",
    "        model = RandNE(dimensions=32, seed=random_seed)\n",
    "        model.fit(G)\n",
    "        embed_RandNE = model.get_embedding()\n",
    "        save_data(embed_RandNE, graph_name, 'embed_RandNE')\n",
    "    similarity_list = [dot(embed_RandNE[u], embed_RandNE[v]) for u, v in tqdm(edges, desc='similarity')]\n",
    "    edges_sorted_sim = [x for _, x in sorted(zip(similarity_list, edges), reverse=True)]\n",
    "    weights_sorted = list(chain.from_iterable([[i] * (i2Ei_fitted[i] - i2Ei_fitted.get(i + 1, 0))\n",
    "                                               for i in reversed(i_list)]))\n",
    "    e2predWeight = dict(zip(edges_sorted_sim, weights_sorted))\n",
    "    for i_layer in range(2, 6):\n",
    "        # sampling\n",
    "        strong_edges = [e for e in edges if e2predWeight[e] >= i_layer]\n",
    "        G = nx.from_edgelist(strong_edges)\n",
    "        edges = strong_edges[:]\n",
    "        # save\n",
    "        with open(p_experiments_SEB / f'{graph_name}.G_{i_layer}', 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "    with open(p_experiments_SEB / f'{graph_name}.e2predWeight', 'wb') as f:\n",
    "        pickle.dump(e2predWeight, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PEB (probility-embedding)\n",
    "\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_PEB = p_experiments / 'PEB'\n",
    "p_experiments_PEB.mkdir(exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "for graph_name in graphs_sorted_m:\n",
    "    print(graph_name)\n",
    "    Ei_list = []\n",
    "    for i_layer in range(2, 6):\n",
    "        G = load_data(graph_name, 'layers', layer_index=i_layer)\n",
    "        Ei_list.append(G.number_of_edges())\n",
    "    i_list = list(range(2, 6))\n",
    "    X = np.array(i_list)\n",
    "    Y = np.array(Ei_list)\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(np.log(X).reshape(-1, 1), np.log(Y))\n",
    "    slope = reg.coef_[0]\n",
    "    intercept = reg.intercept_\n",
    "    # print(f'fitting slope = {slope:.3f}, intercept = {intercept:.3f}')\n",
    "    Y_fitted = np.exp(reg.predict(np.log(X).reshape(-1, 1)))\n",
    "    Y_fitted = Y_fitted.reshape(-1)\n",
    "    i2Ei_fitted = {i: round(Ei_fitted) for i, Ei_fitted in zip(i_list, Y_fitted)}\n",
    "    i2Ei_fitted[1] = g2nm[graph_name][1]\n",
    "    edges = load_data(graph_name, 'edges')\n",
    "    try:\n",
    "        embed_RandNE = load_data(graph_name, 'embed_RandNE')\n",
    "    except:\n",
    "        model = RandNE(dimensions=32, seed=random_seed)\n",
    "        model.fit(G)\n",
    "        embed_RandNE = model.get_embedding()\n",
    "        save_data(embed_RandNE, graph_name, 'embed_RandNE')\n",
    "    similarity_list = [math.exp(dot(embed_RandNE[u] / 1000, embed_RandNE[v] / 1000))\n",
    "                       for u, v in tqdm(edges, desc='similarity')]\n",
    "    edges_sorted_sim = [x for _, x in sorted(zip(similarity_list, edges), reverse=True)]\n",
    "    e2predWeight = {e: 1 for e in edges}\n",
    "    sim_sum = sum(similarity_list)\n",
    "    similarity_list_normed = [s / sim_sum for s in similarity_list]\n",
    "    indices = list(range(len(edges)))\n",
    "    for i_layer in range(2, 6):\n",
    "        # sampling\n",
    "        strong_edges = []\n",
    "        strong_indices = []\n",
    "        total_prob = 0.\n",
    "        similarity_list_normed = []\n",
    "        for i in tqdm(indices):\n",
    "            sim_i = similarity_list_normed[i]\n",
    "            total_prob += sim_i\n",
    "            similarity_list_normed.append(sim_i)\n",
    "        sim_sum_wt = sum(similarity_list_normed)\n",
    "        similarity_list_normed = [s / sim_sum_wt for s in similarity_list_normed]\n",
    "        print(sum(similarity_list_normed), min(similarity_list_normed), max(similarity_list_normed))\n",
    "        strong_indices = np.random.choice(len(similarity_list_normed), i2Ei_fitted[wt + 1],\n",
    "                                          p=similarity_list_normed)\n",
    "        strong_edges = [edges[i] for i in strong_indices]\n",
    "        G = nx.from_edgelist(strong_edges)\n",
    "        indices = strong_indices[:]\n",
    "        # save\n",
    "        with open(p_experiments_PEB / f'{graph_name}.G_{i_layer}', 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "    with open(p_experiments_PEB / f'{graph_name}.e2predWeight', 'wb') as f:\n",
    "        pickle.dump(e2predWeight, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# RFF (random forest-feature)\n",
    "\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_RFF = p_experiments / 'RFF'\n",
    "p_experiments_RFF.mkdir(exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "metric_names = [\n",
    "    'CN',\n",
    "    'SA',\n",
    "    'JC',\n",
    "    'HP',\n",
    "    'HD',\n",
    "    'SI',\n",
    "    'LI',\n",
    "    'AA',\n",
    "    'RA',\n",
    "    'PA',\n",
    "    'FM',\n",
    "    'DL',\n",
    "    'EC',\n",
    "    'LP',\n",
    "]\n",
    "\n",
    "for graph_name in graphs_sorted_m:\n",
    "    print(graph_name)\n",
    "    edges = load_data(graph_name, 'edges')\n",
    "    weights = load_data(graph_name, 'weights')\n",
    "    try:\n",
    "        part2indices = load_data(graph_name, 'part2indices')\n",
    "    except:\n",
    "        edges_and_weights = load_data(graph_name, 'edges_and_weights')\n",
    "        weight2indices = defaultdict(list)\n",
    "        for i, (u, v, w) in enumerate(edge_weights):\n",
    "            if w >= 5:\n",
    "                w = 5\n",
    "            weight2indices[w].append(i)\n",
    "        train_val_test_mask = [None] * len(edge_weights)\n",
    "        part2indices = defaultdict(list)\n",
    "        for w, indices_w in weight2indices.items():\n",
    "            val_size = train_size = len(indices_w) // 10\n",
    "            indices_perm = list(np.random.permutation(indices_w))\n",
    "            part2indices['train'] += indices_perm[:train_size]\n",
    "            part2indices['val'] += indices_perm[train_size:train_size + val_size]\n",
    "            part2indices['test'] += indices_perm[train_size + val_size:]\n",
    "        part2indices = dict(part2indices)\n",
    "        save_data(part2indices, graph_name, 'part2indices')\n",
    "    weights = [int(min(5, w)) for w in weights]\n",
    "    feature_matrix = []\n",
    "    for m in metric_names:\n",
    "        feature_matrix.append(load_data(graph_name, f'{m}_list'))\n",
    "    feature_matrix = np.transpose(feature_matrix)\n",
    "    indices_train = part2indices['train']\n",
    "    X = feature_matrix_train = feature_matrix[indices_train]\n",
    "    y = weights_train = [weights[i] for i in indices_train]\n",
    "    clf = RandomForestClassifier(n_estimators=32, max_depth=2, random_state=42)\n",
    "    clf.fit(X, y)\n",
    "    weights_pred = clf.predict(feature_matrix)\n",
    "    e2predWeight = dict(zip(edges, weights_pred))\n",
    "    for i_layer in range(2, 6):\n",
    "        # sampling\n",
    "        strong_edges = [e for e in edges if e2predWeight[e] >= i_layer]\n",
    "        G = nx.from_edgelist(strong_edges)\n",
    "        edges = strong_edges[:]\n",
    "        # save\n",
    "        with open(p_experiments_RFF / f'{graph_name}.G_{i_layer}', 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "    with open(p_experiments_RFF / f'{graph_name}.e2predWeight', 'wb') as f:\n",
    "        pickle.dump(e2predWeight, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# NEB (neural network-embedding)\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_NEB = p_experiments / 'NEB'\n",
    "p_experiments_NEB.mkdir(exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "class edge_classifier(nn.Module):\n",
    "    def __init__(self, dim_input):\n",
    "        super().__init__()\n",
    "        self.bilinear = nn.Bilinear(dim_input, dim_input, 5)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        return self.bilinear(u, v)\n",
    "\n",
    "\n",
    "def get_embedding(base_path, _path):\n",
    "    return load_data(_path, 'embed_RandNE')\n",
    "\n",
    "\n",
    "def read_file(base_path, _path):\n",
    "    _file = load_data(_path, 'edges_and_weights')\n",
    "    e_list = list()\n",
    "    w_list = list()\n",
    "    for (src, dst, w) in tqdm(_file):\n",
    "        if w >= 5:\n",
    "            w = 5\n",
    "        e_list.append((src, dst))\n",
    "        # weight 1 2 3 4 5 --> 0 1 2 3 4\n",
    "        w_list.append(int(w) - 1)\n",
    "    return e_list, w_list\n",
    "\n",
    "\n",
    "def get_split(base_path, _path):\n",
    "    _file = load_data(_path, 'part2indices')\n",
    "    print(len(_file['train']), len(_file['val']), len(_file['test']))\n",
    "    return _file['train'], _file['val'], _file['test']\n",
    "\n",
    "\n",
    "def get_loader(_list, _b_size, shuffle):\n",
    "    if shuffle:\n",
    "        random.shuffle(_list)\n",
    "    _loader = []\n",
    "    _temp = len(_list) // _b_size\n",
    "\n",
    "    for _i in range(_temp):\n",
    "        _loader.append(_list[_i * _b_size:(_i + 1) * _b_size])\n",
    "    _loader.append(_list[-(len(_list) % _b_size):])\n",
    "\n",
    "    return _loader\n",
    "\n",
    "\n",
    "def train(BASE_PATH, dataset_name, batch_size, num_epochs=200, _lr=0.0001, default_device=\"cuda:0\"):\n",
    "    emb = get_embedding(BASE_PATH, dataset_name)\n",
    "    edge_list, w_list = read_file(BASE_PATH, dataset_name)\n",
    "    #     len(edge_list), len(w_list)\n",
    "    train_split, val_split, test_split = get_split(BASE_PATH, dataset_name)\n",
    "\n",
    "    train_loader = get_loader(train_split, batch_size, shuffle=True)\n",
    "    val_loader = get_loader(val_split, batch_size, shuffle=False)\n",
    "    test_loader = get_loader(test_split, batch_size, shuffle=False)\n",
    "\n",
    "    print(\"Len train/val/test loader {} {} {}\".format(len(train_loader), len(val_loader), len(test_loader)))\n",
    "\n",
    "    edge_list = torch.tensor(edge_list)\n",
    "    w_list = torch.tensor(w_list).to(default_device)\n",
    "    ec = edge_classifier(32).to(default_device)\n",
    "    opt = torch.optim.Adam(ec.parameters(), lr=_lr)\n",
    "\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "\n",
    "    valid_iter = 5\n",
    "\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = 999\n",
    "    best_epoch = -1\n",
    "\n",
    "    for _epoch in tqdm(range(num_epochs)):\n",
    "        train_loader = get_loader(train_split, batch_size, shuffle=True)\n",
    "        for _step, e_ids in enumerate(train_loader):\n",
    "            ec.train()\n",
    "            opt.zero_grad()\n",
    "            '''\n",
    "                _batch: list of edge indices\n",
    "            '''\n",
    "            _src = edge_list[e_ids][:, 0].tolist()\n",
    "            _dst = edge_list[e_ids][:, 1].tolist()\n",
    "            _src = torch.tensor(emb[_src]).float().to(default_device)\n",
    "            _dst = torch.tensor(emb[_dst]).float().to(default_device)\n",
    "            #             print(_src, _dst)\n",
    "            preds = ec(_src, _dst).to(default_device)\n",
    "            _w = w_list[e_ids]\n",
    "\n",
    "            loss = F.cross_entropy(preds, _w)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_loss = 0.\n",
    "        #         print(\"Epoch {} Train loss {}\".format(_step, train_loss))\n",
    "\n",
    "        if _epoch % valid_iter == 0:\n",
    "            for e_ids in val_loader:\n",
    "                ec.eval()\n",
    "                _src = edge_list[e_ids][:, 0].tolist()\n",
    "                _dst = edge_list[e_ids][:, 1].tolist()\n",
    "                _src = torch.tensor(emb[_src]).float().to(default_device)\n",
    "                _dst = torch.tensor(emb[_dst]).float().to(default_device)\n",
    "                preds = ec(_src, _dst).to(default_device)\n",
    "                _w = w_list[e_ids]\n",
    "\n",
    "                loss = F.cross_entropy(preds, _w)\n",
    "                valid_loss += loss.item()\n",
    "            valid_loss /= len(val_loader)\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_epoch = _epoch\n",
    "                best_valid_loss = valid_loss\n",
    "                best_model = copy.deepcopy(ec)\n",
    "\n",
    "            valid_loss_list.append(valid_loss)\n",
    "            valid_loss = 0.\n",
    "    #     print(\"Epoch {} Val loss {}\".format(_epoch, valid_loss))\n",
    "\n",
    "    test_loss = 0.\n",
    "    test_src_list = []\n",
    "    test_dst_list = []\n",
    "    test_pred_list = []\n",
    "    for e_ids in test_loader:\n",
    "        best_model.eval()\n",
    "        _src = edge_list[e_ids][:, 0].tolist()\n",
    "        _dst = edge_list[e_ids][:, 1].tolist()\n",
    "        test_src_list += _src\n",
    "        test_dst_list += _dst\n",
    "        _src = torch.tensor(emb[_src]).float().to(default_device)\n",
    "        _dst = torch.tensor(emb[_dst]).float().to(default_device)\n",
    "        preds = best_model(_src, _dst).to(default_device)\n",
    "        test_pred_list += (torch.argmax(preds, dim=-1) + 1).tolist()\n",
    "        _w = w_list[e_ids]\n",
    "\n",
    "        loss = F.cross_entropy(preds, _w)\n",
    "        test_loss += loss.item()\n",
    "    test_loss /= len(test_loader)\n",
    "    plt.plot(range(num_epochs), train_loss_list)\n",
    "    plt.title(\"Train loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(num_epochs // valid_iter), valid_loss_list)\n",
    "    plt.title(\"Validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    print(\"Best epoch {} Val loss {} Test loss {}\".format(best_epoch, best_valid_loss, test_loss))\n",
    "\n",
    "    '''\n",
    "        predict for train/valid set\n",
    "    '''\n",
    "    tv_src_list = []\n",
    "    tv_dst_list = []\n",
    "    tv_pred_list = []\n",
    "    for e_ids in (train_loader + val_loader):\n",
    "        best_model.eval()\n",
    "        _src = edge_list[e_ids][:, 0].tolist()\n",
    "        _dst = edge_list[e_ids][:, 1].tolist()\n",
    "        tv_src_list += _src\n",
    "        tv_dst_list += _dst\n",
    "        _src = torch.tensor(emb[_src]).float().to(default_device)\n",
    "        _dst = torch.tensor(emb[_dst]).float().to(default_device)\n",
    "        preds = best_model(_src, _dst).to(default_device)\n",
    "        tv_pred_list += (torch.argmax(preds, dim=-1) + 1).tolist()\n",
    "\n",
    "    return best_model, tv_src_list, tv_dst_list, tv_pred_list, test_src_list, test_dst_list, test_pred_list\n",
    "\n",
    "\n",
    "def save_result(dataset_name, tv_src_list, tv_dst_list, tv_pred_list, test_src_list, test_dst_list, test_pred_list):\n",
    "    len_tv = len(tv_src_list)\n",
    "    len_test = len(test_src_list)\n",
    "    p_raw_results = p_experiments_NEB / 'raw_results'\n",
    "    with open(p_raw_results / f'{dataset_name}.txt', 'w+') as f:\n",
    "        for _idx in range(len_tv):\n",
    "            f.write(str(tv_src_list[_idx]) + \",\" + str(tv_dst_list[_idx]) + \",\" + str(tv_pred_list[_idx]) + \"\\n\")\n",
    "        for _idx in range(len_test):\n",
    "            f.write(str(test_src_list[_idx]) + \",\" + str(test_dst_list[_idx]) + \",\" + str(test_pred_list[_idx]) + \"\\n\")\n",
    "\n",
    "\n",
    "BASE_PATH = \"./\"\n",
    "for graph_name in graphs_sorted_m:\n",
    "    print(graph_name)\n",
    "    best_model, tv_src_list, tv_dst_list, tv_pred_list,\n",
    "    test_src_list, test_dst_list, test_pred_list = train(BASE_PATH,\n",
    "                                                         graph_name,\n",
    "                                                         batch_size=128,\n",
    "                                                         num_epochs=200,\n",
    "                                                         _lr=0.0005,\n",
    "                                                         default_device=\"cuda:1\")\n",
    "    save_result(graph_name, tv_src_list, tv_dst_list, tv_pred_list, test_src_list, test_dst_list, test_pred_list)\n",
    "    # p_experiments_NEB / 'raw_results' / f'{graph_name}.txt'\n",
    "    with open(p_experiments_NEB / 'raw_results' / f'{graph_name}.txt') as f:\n",
    "        dd = f.readlines()\n",
    "    e2predWeight = dict()\n",
    "    for d in dd:\n",
    "        u, v, w = map(int, d.split(','))\n",
    "        e2predWeight[min_max_tuple(u, v)] = w\n",
    "\n",
    "    for i_layer in range(2, 6):\n",
    "        # sampling\n",
    "        strong_edges = [(u, v) for u, v in edges if e2predWeight[min_max_tuple(u, v)] >= i_layer]\n",
    "        G = nx.from_edgelist(strong_edges)\n",
    "        edges = strong_edges[:]\n",
    "        # save\n",
    "        with open(p_experiments_NEB / f'{graph_name}.G_{i_layer}', 'wb') as f:\n",
    "            pickle.dump(G, f)\n",
    "    with open(p_experiments_NEB / f'{graph_name}.e2predWeight', 'wb') as f:\n",
    "        pickle.dump(e2predWeight, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import stats, pearsonr, ks_2samp\n",
    "from netrd.distance import NetSimile\n",
    "\n",
    "# check the results\n",
    "\n",
    "p_experiments = Path('experiments')\n",
    "p_experiments.mkdir(exist_ok=True)\n",
    "\n",
    "p_experiments_results = p_experiments / 'results'\n",
    "p_experiments_results.mkdir(exist_ok=True)\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "methods = [\n",
    "    'PEAR',\n",
    "    'PRD',\n",
    "    'CSN',\n",
    "    'SEB',\n",
    "    'PEB',\n",
    "    'RFF',\n",
    "    'NEB',\n",
    "]\n",
    "\n",
    "with open(p_experiments_results / 'results.txt', 'a+') as f_out:\n",
    "    for graph_name in graphs_sorted_m:\n",
    "        if 'coauth' not in graph_name:\n",
    "            continue\n",
    "        print(graph_name)\n",
    "        for i_layer in range(2, 6):\n",
    "            print(f'{graph_name}, layer-{i_layer}')\n",
    "            print(f'{graph_name}, layer-{i_layer}', file=f_out)\n",
    "            G_i = load_data(graph_name, 'layers', layer_index=i_layer)\n",
    "            v2Nv_GT = dict()\n",
    "            v2dv_GT = dict()\n",
    "            for v in G_i:\n",
    "                Nv = set(G_i[v])\n",
    "                v2Nv_GT[v] = Nv\n",
    "                v2dv_GT[v] = len(Nv)\n",
    "            degrees_GT = list(v2dv_GT.values())\n",
    "            cns_GT = [len(v2Nv_GT[u] & v2Nv_GT[v]) for u, v in iter_edges(G_i)]\n",
    "            acc_GT = nx.average_clustering(G_i)\n",
    "            print('GT acc =', acc_GT)\n",
    "            print('GT acc =', acc_GT, file=f_out)\n",
    "            for method in methods:\n",
    "                p_experiments_method = p_experiments / method\n",
    "                for random_seed in [1, 2, 3] if method == 'PEAR' else [42]:\n",
    "                    p_Gi = p_experiments_method / f'{graph_name}-seed{random_seed}.G_{i_layer}' if method == 'PEAR' \\\n",
    "                        else p_experiments_method / f'{graph_name}.G_{i_layer}'\n",
    "                    with open(p_Gi, 'rb') as f:\n",
    "                        G_i_generated: nx.Graph = pickle.load(f)\n",
    "                    v2Nv = dict()\n",
    "                    v2dv = dict()\n",
    "                    for v in G_i_generated:\n",
    "                        Nv = set(G_i_generated[v])\n",
    "                        v2Nv[v] = Nv\n",
    "                        v2dv[v] = len(Nv)\n",
    "                    ks_stat, _ = ks_2samp(degrees_GT, list(v2dv.values()))\n",
    "                    print('KSND', f'seed {random_seed}', ks_stat)\n",
    "                    print('KSND', f'seed {random_seed}', ks_stat, file=f_out)\n",
    "\n",
    "                    cns = [len(v2Nv[u] & v2Nv[v]) for u, v in iter_edges(G_i_generated)]\n",
    "                    ks_stat, _ = ks_2samp(cns_GT, cns)\n",
    "                    print('KSCN', f'seed {random_seed}', ks_stat)\n",
    "                    print('KSCN', f'seed {random_seed}', ks_stat, file=f_out)\n",
    "\n",
    "                    acc = nx.average_clustering(G_i_generated)\n",
    "                    print('DACC', f'seed {random_seed}', abs(acc - acc_GT))\n",
    "                    print('DACC', f'seed {random_seed}', abs(acc - acc_GT), file=f_out)\n",
    "\n",
    "                    if graph_name == 'sx-stackoverflow':\n",
    "                        continue\n",
    "                    dist_obj = NetSimile()\n",
    "                    distance = dist_obj.dist(G_i_generated, G_i)\n",
    "                    print(f'NetSimile', f'seed {random_seed}', distance)\n",
    "                    print(f'NetSimile', f'seed {random_seed}', distance, file=f_out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}